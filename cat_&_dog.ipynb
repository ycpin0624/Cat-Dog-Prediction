{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ycpin0624/Cat-Dog-Prediction/blob/main/cat_%26_dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4vEN_8rzZUe",
        "outputId": "6a45f9d8-fec3-42ee-9490-299036786082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O /tmp/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw3IcI8bw_Ii",
        "outputId": "381339e7-8d4c-4f91-d463-1b368a3fe29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-10 02:22:45--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.5.128, 64.233.184.128, 74.125.140.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.5.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   121MB/s    in 0.5s    \n",
            "\n",
            "2022-05-10 02:22:45 (121 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE9EJK9RLDGw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/MyDrive/image/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/drive/MyDrive/image/cats_dogs')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQUV2Snn3ONM"
      },
      "outputs": [],
      "source": [
        "# 取得資料集路徑\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/tmp/cats_dogs/cats_and_dogs_filtered/'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')           # 取得訓練用貓圖片的路徑\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')           # 取得訓練用狗圖片的路徑\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')      # 取得驗證用貓圖片的路徑\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')      # 取得驗證用狗圖片的路徑\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)             # 取得訓練用所有貓圖片\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)             # 取得訓練用所有狗圖片\n",
        "validation_cat_fnames = os.listdir(validation_cats_dir)        # 取得驗證用所有貓圖片\n",
        "validation_dog_fnames = os.listdir(validation_dogs_dir)        # 取得驗證用所有狗圖片"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5TxZ76MDg0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b2059b-b2b4-4a22-ee7b-f6937bc64894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1800 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 224, 224, 3)\n",
            "(8, 2)\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258080768/258076736 [==============================] - 1s 0us/step\n",
            "258088960/258076736 [==============================] - 1s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " efficientnetb7 (Functional)  (None, 2560)             64097687  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1311232   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 65,409,945\n",
            "Trainable params: 1,312,258\n",
            "Non-trainable params: 64,097,687\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:106: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "28/28 [==============================] - 64s 1s/step - loss: 0.4891 - acc: 0.8170 - val_loss: 0.2950 - val_acc: 0.9583\n",
            "Epoch 2/15\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.2304 - acc: 0.9777 - val_loss: 0.1903 - val_acc: 1.0000\n",
            "Epoch 3/15\n",
            "28/28 [==============================] - 23s 830ms/step - loss: 0.1364 - acc: 0.9866 - val_loss: 0.2009 - val_acc: 0.9583\n",
            "Epoch 4/15\n",
            "28/28 [==============================] - 20s 704ms/step - loss: 0.1053 - acc: 0.9821 - val_loss: 0.1156 - val_acc: 1.0000\n",
            "Epoch 5/15\n",
            "28/28 [==============================] - 19s 703ms/step - loss: 0.0839 - acc: 0.9821 - val_loss: 0.0491 - val_acc: 1.0000\n",
            "Epoch 6/15\n",
            "28/28 [==============================] - 16s 589ms/step - loss: 0.0757 - acc: 0.9866 - val_loss: 0.0180 - val_acc: 1.0000\n",
            "Epoch 7/15\n",
            "28/28 [==============================] - 15s 552ms/step - loss: 0.0661 - acc: 0.9821 - val_loss: 0.0344 - val_acc: 1.0000\n",
            "Epoch 8/15\n",
            "28/28 [==============================] - 14s 512ms/step - loss: 0.0495 - acc: 0.9955 - val_loss: 0.0271 - val_acc: 1.0000\n",
            "Epoch 9/15\n",
            "28/28 [==============================] - 15s 512ms/step - loss: 0.0517 - acc: 0.9911 - val_loss: 0.0567 - val_acc: 1.0000\n",
            "Epoch 10/15\n",
            "28/28 [==============================] - 11s 401ms/step - loss: 0.0438 - acc: 0.9821 - val_loss: 0.0435 - val_acc: 1.0000\n",
            "Epoch 11/15\n",
            "28/28 [==============================] - 11s 399ms/step - loss: 0.0547 - acc: 0.9821 - val_loss: 0.0243 - val_acc: 1.0000\n",
            "Epoch 12/15\n",
            "28/28 [==============================] - 12s 426ms/step - loss: 0.0366 - acc: 0.9911 - val_loss: 0.0283 - val_acc: 1.0000\n",
            "Epoch 13/15\n",
            "28/28 [==============================] - 9s 314ms/step - loss: 0.0320 - acc: 0.9955 - val_loss: 0.2642 - val_acc: 0.9167\n",
            "Epoch 14/15\n",
            "28/28 [==============================] - 9s 314ms/step - loss: 0.0355 - acc: 0.9911 - val_loss: 0.0320 - val_acc: 1.0000\n",
            "Epoch 15/15\n",
            "28/28 [==============================] - 9s 303ms/step - loss: 0.0358 - acc: 0.9911 - val_loss: 0.0680 - val_acc: 0.9583\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.0087 - acc: 1.0000\n",
            "[0.008662586100399494, 1.0]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout, Input, Conv2D, MaxPooling2D\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.xception import Xception\n",
        "from tensorflow.keras import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD\n",
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "import keras.optimizers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pathlib\n",
        "\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "image_size = (224, 224)\n",
        "batch_size = 8\n",
        "\n",
        "data_dir = pathlib.Path(\"/content/drive/MyDrive/image/cats_dogs/cats_and_dogs_filtered/train/\")\n",
        "test_data_dir = pathlib.Path(\"/content/drive/MyDrive/image/cats_dogs/cats_and_dogs_filtered/validation/\")\n",
        "\n",
        "# 訓練資料\n",
        "train_datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    rotation_range=20,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "train_ds = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', # class name 轉為 0/1\n",
        "    shuffle=True,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_ds = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', # class name 轉為 0/1\n",
        "    shuffle=True,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "print(train_ds.class_indices)\n",
        "\n",
        "# 驗證資料\n",
        "datagen = ImageDataGenerator()\n",
        "test_ds = datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# 檢查資料\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n",
        "\n",
        "def define_model():\n",
        "  conv_base = tf.keras.applications.EfficientNetB7(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3), pooling='avg')\n",
        "  conv_base.trainable = False\n",
        "  # conv_base.summary()\n",
        "  # flat1 = Flatten()(model.layers[-1].output)\n",
        "  # class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "  # output = Dense(2, activation='sigmoid')(class1)\n",
        "  # model = Model(inputs=model.inputs, outputs=output)\n",
        "  # opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "  # model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
        "      \n",
        "  model = Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(layers.Dense(2, activation='softmax'))\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0005/10), loss='categorical_crossentropy', metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "model = define_model()\n",
        "model.summary()\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_ds,\n",
        "    epochs=15,\n",
        "    steps_per_epoch=len(train_ds)//batch_size,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=len(val_ds)//batch_size)\n",
        "\n",
        "model.save('/content/drive/MyDrive/final_model.h5')\n",
        "\n",
        "results = model.evaluate(test_ds, batch_size=batch_size, steps=15)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "import pathlib\n",
        "\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "image_size = (224, 224)\n",
        "batch_size = 8\n",
        "\n",
        "test_data_dir = pathlib.Path(\"/content/drive/MyDrive/image/cats_dogs/cats_and_dogs_filtered/validation/\")\n",
        "\n",
        "datagen = ImageDataGenerator()\n",
        "test_ds = datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "print(test_ds.class_indices)\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/final_model.h5')\n",
        "_, acc = model.evaluate(test_ds, batch_size=batch_size, steps=15)\n",
        "print(\"acc:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FMz5m74RDE1",
        "outputId": "7e8eae53-7799-4756-b367-8289fefbd406"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "{'cats': 0, 'dogs': 1}\n",
            "15/15 [==============================] - 12s 343ms/step - loss: 0.0191 - acc: 1.0000\n",
            "acc: 1.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cat & dog",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}