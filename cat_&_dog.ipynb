{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4vEN_8rzZUe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw3IcI8bw_Ii",
        "outputId": "8bafade3-76e0-4da5-ad95-b5cb5f6677a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/tmp/cats_and_dogs_filtered.zip: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O /content/drive/MyDrive/tmp/cats_and_dogs_filtered.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE9EJK9RLDGw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/MyDrive/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/drive/MyDrive/tmp/cats_dogs')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQUV2Snn3ONM"
      },
      "outputs": [],
      "source": [
        "# 取得資料集路徑\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/tmp/cats_dogs/cats_and_dogs_filtered/'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')           # 取得訓練用貓圖片的路徑\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')           # 取得訓練用狗圖片的路徑\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')      # 取得驗證用貓圖片的路徑\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')      # 取得驗證用狗圖片的路徑\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)             # 取得訓練用所有貓圖片\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)             # 取得訓練用所有狗圖片\n",
        "validation_cat_fnames = os.listdir(validation_cats_dir)        # 取得驗證用所有貓圖片\n",
        "validation_dog_fnames = os.listdir(validation_dogs_dir)        # 取得驗證用所有狗圖片"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5TxZ76MDg0Q",
        "outputId": "43456d09-d4ba-4cd9-a896-6d51af22b4a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1800 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n",
            "{'cats': 0, 'dogs': 1}\n",
            "Found 1000 images belonging to 2 classes.\n",
            "(4, 224, 224, 3)\n",
            "(4, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnetb7 (Functional)  (None, 2560)             64097687  \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 512)               1311232   \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 65,409,945\n",
            "Trainable params: 1,312,258\n",
            "Non-trainable params: 64,097,687\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:106: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "112/112 [==============================] - 222s 2s/step - loss: 0.2612 - acc: 0.9330 - val_loss: 0.1119 - val_acc: 0.9583\n",
            "Epoch 2/20\n",
            "112/112 [==============================] - 194s 2s/step - loss: 0.0954 - acc: 0.9821 - val_loss: 0.0766 - val_acc: 0.9792\n",
            "Epoch 3/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0553 - acc: 0.9866 - val_loss: 0.0444 - val_acc: 0.9792\n",
            "Epoch 4/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0493 - acc: 0.9888 - val_loss: 0.0803 - val_acc: 0.9583\n",
            "Epoch 5/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0241 - acc: 0.9955 - val_loss: 0.0427 - val_acc: 0.9792\n",
            "Epoch 6/20\n",
            "112/112 [==============================] - 197s 2s/step - loss: 0.0353 - acc: 0.9933 - val_loss: 0.0292 - val_acc: 0.9792\n",
            "Epoch 7/20\n",
            "112/112 [==============================] - 196s 2s/step - loss: 0.0285 - acc: 0.9888 - val_loss: 0.0522 - val_acc: 0.9792\n",
            "Epoch 8/20\n",
            "112/112 [==============================] - 198s 2s/step - loss: 0.0193 - acc: 0.9933 - val_loss: 0.0114 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0339 - acc: 0.9888 - val_loss: 0.0060 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "112/112 [==============================] - 196s 2s/step - loss: 0.0126 - acc: 0.9978 - val_loss: 0.0040 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0168 - acc: 0.9933 - val_loss: 0.0954 - val_acc: 0.9792\n",
            "Epoch 12/20\n",
            "112/112 [==============================] - 196s 2s/step - loss: 0.0149 - acc: 0.9978 - val_loss: 0.1227 - val_acc: 0.9583\n",
            "Epoch 13/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0211 - acc: 0.9933 - val_loss: 0.0143 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0361 - acc: 0.9866 - val_loss: 0.0366 - val_acc: 0.9792\n",
            "Epoch 15/20\n",
            "112/112 [==============================] - 196s 2s/step - loss: 0.0171 - acc: 0.9955 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0251 - acc: 0.9888 - val_loss: 0.0301 - val_acc: 0.9792\n",
            "Epoch 17/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0173 - acc: 0.9955 - val_loss: 0.0098 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "112/112 [==============================] - 196s 2s/step - loss: 0.0178 - acc: 0.9955 - val_loss: 0.0743 - val_acc: 0.9792\n",
            "Epoch 19/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0088 - acc: 0.9978 - val_loss: 0.0350 - val_acc: 0.9792\n",
            "Epoch 20/20\n",
            "112/112 [==============================] - 195s 2s/step - loss: 0.0158 - acc: 0.9933 - val_loss: 0.0048 - val_acc: 1.0000\n",
            "250/250 [==============================] - 354s 1s/step - loss: 0.0232 - acc: 0.9940\n",
            "[0.02316121943295002, 0.9940000176429749]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout, Input, Conv2D, MaxPooling2D\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.xception import Xception\n",
        "from tensorflow.keras import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD\n",
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "import keras.optimizers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pathlib\n",
        "\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "image_size = (224, 224)\n",
        "batch_size = 4\n",
        "\n",
        "data_dir = pathlib.Path(\"/content/drive/MyDrive/tmp/cats_dogs/cats_and_dogs_filtered/train/\")\n",
        "test_data_dir = pathlib.Path(\"/content/drive/MyDrive/tmp/cats_dogs/cats_and_dogs_filtered/validation/\")\n",
        "\n",
        "# 訓練資料\n",
        "train_datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    rotation_range=20,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "train_ds = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', # class name 轉為 0/1\n",
        "    shuffle=True,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_ds = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', # class name 轉為 0/1\n",
        "    shuffle=True,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "print(train_ds.class_indices)\n",
        "\n",
        "# 驗證資料\n",
        "datagen = ImageDataGenerator()\n",
        "test_ds = datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# 檢查資料\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n",
        "\n",
        "def define_model():\n",
        "  conv_base = tf.keras.applications.EfficientNetB7(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3), pooling='avg')\n",
        "  conv_base.trainable = False\n",
        "  # conv_base.summary()\n",
        "  # flat1 = Flatten()(model.layers[-1].output)\n",
        "  # class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "  # output = Dense(2, activation='sigmoid')(class1)\n",
        "  # model = Model(inputs=model.inputs, outputs=output)\n",
        "  # opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "  # model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
        "      \n",
        "  model = Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(layers.Dense(512, activation='relu'))\n",
        "  model.add(layers.Dense(2, activation='softmax'))\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0005/10), loss='categorical_crossentropy', metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "model = define_model()\n",
        "model.summary()\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_ds,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(train_ds)//batch_size,\n",
        "    validation_data=val_ds, \n",
        "    validation_steps=len(val_ds)//batch_size)\n",
        "\n",
        "model.save('/content/drive/MyDrive/final_model.h5')\n",
        "\n",
        "results = model.evaluate(test_ds, batch_size=batch_size, use_multiprocessing=True)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FMz5m74RDE1",
        "outputId": "6f119f37-4b69-4be7-a1a9-5134a9376754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "{'cats': 0, 'dogs': 1}\n",
            "250/250 [==============================] - 353s 1s/step - loss: 0.0232 - acc: 0.9940\n",
            "acc: 0.9940000176429749\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "import pathlib\n",
        "\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "image_size = (224, 224)\n",
        "batch_size = 4\n",
        "\n",
        "test_data_dir = pathlib.Path(\"/content/drive/MyDrive/tmp/cats_dogs/cats_and_dogs_filtered/validation/\")\n",
        "\n",
        "datagen = ImageDataGenerator()\n",
        "test_ds = datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "print(test_ds.class_indices)\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/final_model.h5')\n",
        "_, acc = model.evaluate(test_ds, batch_size=batch_size, use_multiprocessing=True)\n",
        "print(\"acc:\", acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cat & dog",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}